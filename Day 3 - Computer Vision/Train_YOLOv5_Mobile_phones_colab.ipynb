{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mGmQbAO5pQb"
      },
      "source": [
        "#Install Dependencies\n",
        "\n",
        "_(Remember to choose GPU in Runtime if not already selected. Runtime --> Change Runtime Type --> Hardware accelerator --> GPU)_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GD9gUQpaBxNa"
      },
      "source": [
        "# How to Train YOLOv5 on Custom Objects\n",
        "\n",
        "This tutorial is based on the [YOLOv5 repository](https://github.com/ultralytics/yolov5) by [Ultralytics](https://www.ultralytics.com/). This notebook shows training on **your own custom objects**. \n",
        "\n",
        "### Accompanying Blog Post\n",
        "\n",
        "We recommend that you follow along in this notebook while reading the blog post on [how to train YOLOv5](https://blog.roboflow.ai/how-to-train-yolov5-on-a-custom-dataset/), and [how to train YOLOv5 with deployment to web camera](https://blog.paperspace.com/train-yolov5-custom-data/#set-up-the-code) concurrently.\n",
        "\n",
        "### Steps Covered in this Tutorial\n",
        "\n",
        "In this exercise, we will walk through the steps required to train YOLOv5 on your custom objects. We use a private, AAU-made dataset containing parts of a mock-up mobile phone from our AAU Smart Production Lab.\n",
        "\n",
        "The dataset is already uploaded on Roboflow (as we have seen during the lecture) but you can also download it from PhD moodle (make sure you are already logged in there) to use in your local environment.\n",
        "\n",
        "[Download the dataset from AAU PhD moodle here](https://phd.moodle.aau.dk/mod/resource/view.php?id=18660)\n",
        "\n",
        "In the exercise here, we use the dataset directly from Roboflow. \n",
        "\n",
        "The dataset is free to use for the needs of the course. However, please do not share the dataset with others. \n",
        "\n",
        "You can also use this notebook on your own custom data.\n",
        "\n",
        "To train the detector we take the following steps:\n",
        "\n",
        "* Install YOLOv5 dependencies\n",
        "* Download custom YOLOv5 object detection data\n",
        "* Write our YOLOv5 Training configuration\n",
        "* Run YOLOv5 training\n",
        "* Evaluate YOLOv5 performance\n",
        "* Visualize YOLOv5 training data\n",
        "* Run YOLOv5 inference on test images\n",
        "* Export saved YOLOv5 weights for future inference\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbvMlHd_QwMG",
        "outputId": "9c24e1bd-2500-4a0a-8269-68b032660b06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |▏                               | 10 kB 29.7 MB/s eta 0:00:01\r\u001b[K     |▍                               | 20 kB 17.8 MB/s eta 0:00:01\r\u001b[K     |▋                               | 30 kB 23.8 MB/s eta 0:00:01\r\u001b[K     |▉                               | 40 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |█                               | 51 kB 15.2 MB/s eta 0:00:01\r\u001b[K     |█▎                              | 61 kB 17.7 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 71 kB 18.4 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 81 kB 20.0 MB/s eta 0:00:01\r\u001b[K     |█▉                              | 92 kB 19.2 MB/s eta 0:00:01\r\u001b[K     |██                              | 102 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 112 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |██▌                             | 122 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 133 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |███                             | 143 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 153 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 163 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 174 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 184 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |████                            | 194 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 204 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 215 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 225 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 235 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |█████                           | 245 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 256 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 266 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 276 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 286 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |██████                          | 296 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 307 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 317 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 327 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |███████                         | 337 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |███████                         | 348 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 358 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 368 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 378 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |████████                        | 389 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 399 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 409 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 419 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 430 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 440 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 450 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 460 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 471 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 481 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 491 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 501 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 512 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 522 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 532 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 542 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 552 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 563 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 573 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 583 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 593 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 604 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 614 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 624 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 634 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 645 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 655 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 665 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 675 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 686 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 696 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 706 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 716 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 727 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 737 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 747 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 757 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 768 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 778 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 788 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 798 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 808 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 819 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 829 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 839 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 849 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 860 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 870 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 880 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 890 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 901 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 911 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 921 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 931 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 942 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 952 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 962 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 972 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 983 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 993 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 1.0 MB 16.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 1.0 MB 16.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.0 MB 16.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.0 MB 16.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 1.0 MB 16.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 1.1 MB 16.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 1.1 MB 16.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.1 MB 16.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 1.1 MB 16.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 1.1 MB 16.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 1.1 MB 16.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 1.1 MB 16.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.1 MB 16.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 1.1 MB 16.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 1.1 MB 16.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 1.2 MB 16.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 1.2 MB 16.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.2 MB 16.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 1.2 MB 16.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 1.2 MB 16.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 1.2 MB 16.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 1.2 MB 16.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.2 MB 16.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 1.2 MB 16.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.2 MB 16.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.3 MB 16.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.3 MB 16.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 1.3 MB 16.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 1.3 MB 16.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.3 MB 16.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.3 MB 16.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.3 MB 16.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 1.3 MB 16.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 1.3 MB 16.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 1.4 MB 16.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.4 MB 16.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.4 MB 16.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.4 MB 16.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.4 MB 16.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.4 MB 16.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.4 MB 16.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.4 MB 16.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.4 MB 16.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.4 MB 16.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.5 MB 16.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 1.5 MB 16.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.5 MB 16.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.5 MB 16.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.5 MB 16.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.5 MB 16.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.5 MB 16.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.5 MB 16.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.5 MB 16.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.5 MB 16.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.6 MB 16.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.6 MB 16.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.6 MB 16.0 MB/s \n",
            "\u001b[?25hSetup complete. Using torch 1.12.1+cu113 _CudaDeviceProperties(name='Tesla T4', major=7, minor=5, total_memory=15109MB, multi_processor_count=40)\n"
          ]
        }
      ],
      "source": [
        "# install dependencies as necessary\n",
        "!pip install -qr requirements.txt  # install dependencies (ignore errors)\n",
        "import torch\n",
        "\n",
        "from IPython.display import Image, clear_output  # to display images\n",
        "from utils.downloads import attempt_download  # to download models/datasets\n",
        "\n",
        "# clear_output()\n",
        "print('Setup complete. Using torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ie5uLDH4uzAp",
        "outputId": "dca905da-f038-41cc-a874-e1419d625769"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'yolov5'...\n",
            "remote: Enumerating objects: 14302, done.\u001b[K\n",
            "remote: Counting objects: 100% (60/60), done.\u001b[K\n",
            "remote: Compressing objects: 100% (51/51), done.\u001b[K\n",
            "remote: Total 14302 (delta 26), reused 28 (delta 9), pack-reused 14242\u001b[K\n",
            "Receiving objects: 100% (14302/14302), 13.65 MiB | 23.93 MiB/s, done.\n",
            "Resolving deltas: 100% (9833/9833), done.\n",
            "/content/yolov5\n",
            "HEAD is now at fbe67e4 Fix `OMP_NUM_THREADS=1` for macOS (#8624)\n"
          ]
        }
      ],
      "source": [
        "# clone YOLOv5 repository\n",
        "!git clone https://github.com/ultralytics/yolov5  # clone repo\n",
        "%cd yolov5\n",
        "!git reset --hard fbe67e465375231474a2ad80a4389efc77ecff99"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDIhrBF0sPaM"
      },
      "source": [
        "# Download Correctly Formatted Custom Dataset \n",
        "\n",
        "We'll download our dataset from Roboflow. Use the \"**YOLOv5 PyTorch**\" export format. Note that the Ultralytics implementation calls for a YAML file defining where your training and test data is. The Roboflow export also writes this format for us.\n",
        "\n",
        "To get your data into Roboflow, follow the [Getting Started Guide](https://blog.roboflow.ai/getting-started-with-roboflow/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDeebwqS9JbZ"
      },
      "source": [
        "\n",
        "\n",
        "![YOLOv5 PyTorch export](https://i.imgur.com/5vr9G2u.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Knxi2ncxWffW",
        "outputId": "e35b0d06-c123-4866-f9dd-ab90613374ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 42 kB 767 kB/s \n",
            "\u001b[K     |████████████████████████████████| 178 kB 26.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 54 kB 2.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 67 kB 6.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 138 kB 75.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 145 kB 65.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 62 kB 1.5 MB/s \n",
            "\u001b[?25h  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "upload and label your dataset, and get an API KEY here: https://app.roboflow.com/?model=yolov5&ref=roboflow-yolov5\n"
          ]
        }
      ],
      "source": [
        "#follow the link below to get your download code from from Roboflow\n",
        "!pip install -q roboflow\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(model_format=\"yolov5\", notebook=\"roboflow-yolov5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ug_PhK1oqwQA",
        "outputId": "e19358bb-e68a-4efb-9ba7-4f2d44686c0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/yolov5\n",
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n",
            "Downloading Dataset Version Zip in CV-lecture-test-2 to yolov5pytorch: 100% [20814792 / 20814792] bytes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting Dataset Version Zip to CV-lecture-test-2 in yolov5pytorch:: 100%|██████████| 1056/1056 [00:00<00:00, 1623.61it/s]\n"
          ]
        }
      ],
      "source": [
        "%cd /content/yolov5\n",
        "#after following the link above, recieve python code with these fields filled in\n",
        "#from roboflow import Roboflow\n",
        "#!pip install roboflow\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"dFupJKhsuCbKRD4VEIR4\")\n",
        "project = rf.workspace(\"cv-lecture\").project(\"cv-lecture-test\")\n",
        "dataset = project.version(2).download(\"yolov5\")\n",
        "\n",
        "#rf = Roboflow(api_key=\"YOUR API KEY HERE\")\n",
        "#project = rf.workspace().project(\"YOUR PROJECT\")\n",
        "#dataset = project.version(\"YOUR VERSION\").download(\"yolov5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZ3DmmGQztJj",
        "outputId": "b4851fe8-7ee4-4738-80ab-47963f7ffe2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "names:\n",
            "- Background\n",
            "- BlackCover\n",
            "- BlueCover\n",
            "- BottomCover\n",
            "- Box\n",
            "- PCB\n",
            "- Table\n",
            "- WhiteCover\n",
            "nc: 8\n",
            "roboflow:\n",
            "  license: CC BY 4.0\n",
            "  project: cv-lecture-test\n",
            "  url: https://universe.roboflow.com/project/cv-lecture-test/dataset/2\n",
            "  version: 2\n",
            "  workspace: project\n",
            "test: ../test/images\n",
            "train: CV-lecture-test-2/train/images\n",
            "val: CV-lecture-test-2/valid/images\n"
          ]
        }
      ],
      "source": [
        "# this is the YAML file Roboflow wrote for us that we're loading into this notebook with our data\n",
        "%cat {dataset.location}/data.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwJx-2NHsYxT"
      },
      "source": [
        "# Define Model Configuration and Architecture\n",
        "\n",
        "We will write a yaml script that defines the parameters for our model like the number of classes, anchors, and each layer.\n",
        "\n",
        "You do not need to edit these cells, but you may."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dOPn9wjOAwwK"
      },
      "outputs": [],
      "source": [
        "# define number of classes based on YAML\n",
        "import yaml\n",
        "with open(dataset.location + \"/data.yaml\", 'r') as stream:\n",
        "    num_classes = str(yaml.safe_load(stream)['nc'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Rvt5wilnDyX",
        "outputId": "d7cf2ae1-8d3a-45f9-958e-82361c23d8a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# YOLOv5 🚀 by Ultralytics, GPL-3.0 license\n",
            "\n",
            "# Parameters\n",
            "nc: 80  # number of classes\n",
            "depth_multiple: 0.33  # model depth multiple\n",
            "width_multiple: 0.50  # layer channel multiple\n",
            "anchors:\n",
            "  - [10,13, 16,30, 33,23]  # P3/8\n",
            "  - [30,61, 62,45, 59,119]  # P4/16\n",
            "  - [116,90, 156,198, 373,326]  # P5/32\n",
            "\n",
            "# YOLOv5 v6.0 backbone\n",
            "backbone:\n",
            "  # [from, number, module, args]\n",
            "  [[-1, 1, Conv, [64, 6, 2, 2]],  # 0-P1/2\n",
            "   [-1, 1, Conv, [128, 3, 2]],  # 1-P2/4\n",
            "   [-1, 3, C3, [128]],\n",
            "   [-1, 1, Conv, [256, 3, 2]],  # 3-P3/8\n",
            "   [-1, 6, C3, [256]],\n",
            "   [-1, 1, Conv, [512, 3, 2]],  # 5-P4/16\n",
            "   [-1, 9, C3, [512]],\n",
            "   [-1, 1, Conv, [1024, 3, 2]],  # 7-P5/32\n",
            "   [-1, 3, C3, [1024]],\n",
            "   [-1, 1, SPPF, [1024, 5]],  # 9\n",
            "  ]\n",
            "\n",
            "# YOLOv5 v6.0 head\n",
            "head:\n",
            "  [[-1, 1, Conv, [512, 1, 1]],\n",
            "   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n",
            "   [[-1, 6], 1, Concat, [1]],  # cat backbone P4\n",
            "   [-1, 3, C3, [512, False]],  # 13\n",
            "\n",
            "   [-1, 1, Conv, [256, 1, 1]],\n",
            "   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n",
            "   [[-1, 4], 1, Concat, [1]],  # cat backbone P3\n",
            "   [-1, 3, C3, [256, False]],  # 17 (P3/8-small)\n",
            "\n",
            "   [-1, 1, Conv, [256, 3, 2]],\n",
            "   [[-1, 14], 1, Concat, [1]],  # cat head P4\n",
            "   [-1, 3, C3, [512, False]],  # 20 (P4/16-medium)\n",
            "\n",
            "   [-1, 1, Conv, [512, 3, 2]],\n",
            "   [[-1, 10], 1, Concat, [1]],  # cat head P5\n",
            "   [-1, 3, C3, [1024, False]],  # 23 (P5/32-large)\n",
            "\n",
            "   [[17, 20, 23], 1, Detect, [nc, anchors]],  # Detect(P3, P4, P5)\n",
            "  ]\n"
          ]
        }
      ],
      "source": [
        "#this is the model configuration we will use for our tutorial \n",
        "%cat /content/yolov5/models/yolov5s.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t14hhyqdmw6O"
      },
      "outputs": [],
      "source": [
        "#customize iPython writefile so we can write variables\n",
        "from IPython.core.magic import register_line_cell_magic\n",
        "\n",
        "@register_line_cell_magic\n",
        "def writetemplate(line, cell):\n",
        "    with open(line, 'w') as f:\n",
        "        f.write(cell.format(**globals()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uDxebz13RdRA"
      },
      "outputs": [],
      "source": [
        "%%writetemplate /content/yolov5/models/custom_yolov5s.yaml\n",
        "\n",
        "# parameters\n",
        "nc: {num_classes}  # number of classes\n",
        "depth_multiple: 0.33  # model depth multiple\n",
        "width_multiple: 0.50  # layer channel multiple\n",
        "\n",
        "# anchors\n",
        "anchors:\n",
        "  - [10,13, 16,30, 33,23]  # P3/8\n",
        "  - [30,61, 62,45, 59,119]  # P4/16\n",
        "  - [116,90, 156,198, 373,326]  # P5/32\n",
        "\n",
        "# YOLOv5 backbone\n",
        "backbone:\n",
        "  # [from, number, module, args]\n",
        "  [[-1, 1, Focus, [64, 3]],  # 0-P1/2\n",
        "   [-1, 1, Conv, [128, 3, 2]],  # 1-P2/4\n",
        "   [-1, 3, BottleneckCSP, [128]],\n",
        "   [-1, 1, Conv, [256, 3, 2]],  # 3-P3/8\n",
        "   [-1, 9, BottleneckCSP, [256]],\n",
        "   [-1, 1, Conv, [512, 3, 2]],  # 5-P4/16\n",
        "   [-1, 9, BottleneckCSP, [512]],\n",
        "   [-1, 1, Conv, [1024, 3, 2]],  # 7-P5/32\n",
        "   [-1, 1, SPP, [1024, [5, 9, 13]]],\n",
        "   [-1, 3, BottleneckCSP, [1024, False]],  # 9\n",
        "  ]\n",
        "\n",
        "# YOLOv5 head\n",
        "head:\n",
        "  [[-1, 1, Conv, [512, 1, 1]],\n",
        "   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n",
        "   [[-1, 6], 1, Concat, [1]],  # cat backbone P4\n",
        "   [-1, 3, BottleneckCSP, [512, False]],  # 13\n",
        "\n",
        "   [-1, 1, Conv, [256, 1, 1]],\n",
        "   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n",
        "   [[-1, 4], 1, Concat, [1]],  # cat backbone P3\n",
        "   [-1, 3, BottleneckCSP, [256, False]],  # 17 (P3/8-small)\n",
        "\n",
        "   [-1, 1, Conv, [256, 3, 2]],\n",
        "   [[-1, 14], 1, Concat, [1]],  # cat head P4\n",
        "   [-1, 3, BottleneckCSP, [512, False]],  # 20 (P4/16-medium)\n",
        "\n",
        "   [-1, 1, Conv, [512, 3, 2]],\n",
        "   [[-1, 10], 1, Concat, [1]],  # cat head P5\n",
        "   [-1, 3, BottleneckCSP, [1024, False]],  # 23 (P5/32-large)\n",
        "\n",
        "   [[17, 20, 23], 1, Detect, [nc, anchors]],  # Detect(P3, P4, P5)\n",
        "  ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUOiNLtMP5aG"
      },
      "source": [
        "# Train Custom YOLOv5 Detector\n",
        "\n",
        "### Next, we'll fire off training!\n",
        "\n",
        "\n",
        "Here, we are able to pass a number of arguments:\n",
        "- **img:** define input image size\n",
        "- **batch:** determine batch size\n",
        "- **epochs:** define the number of training epochs. (Note: often, 3000+ are common here!)\n",
        "- **data:** set the path to our yaml file\n",
        "- **cfg:** specify our model configuration\n",
        "- **weights:** specify a custom path to weights. (Note: you can download weights from our GitHub [folder](https://drive.google.com/open?id=1Drs_Aiu7xx6S-ix95f9kNsA6ueKRpN2J))\n",
        "- **name:** result names\n",
        "- **nosave:** only save the final checkpoint\n",
        "- **cache:** cache images for faster training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "1NcFxRcFdJ_O",
        "outputId": "7742e733-f457-4451-86ad-66a8ebe6c53f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/yolov5\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=, cfg=./models/custom_yolov5s.yaml, data=/content/yolov5/CV-lecture-test-2/data.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=300, batch_size=16, imgsz=416, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=ram, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=yolov5s_results, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0m⚠️ YOLOv5 is out of date by 387 commits. Use `git pull` or `git clone https://github.com/ultralytics/yolov5` to update.\n",
            "YOLOv5 🚀 v6.1-306-gfbe67e4 Python-3.8.15 torch-1.12.1+cu113 CUDA:0 (Tesla T4, 15110MiB)\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mWeights & Biases: \u001b[0mrun 'pip install wandb' to automatically track and visualize YOLOv5 🚀 runs (RECOMMENDED)\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
            "Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\n",
            "100% 755k/755k [00:00<00:00, 51.8MB/s]\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      3520  models.common.Focus                     [3, 32, 3]                    \n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     19904  models.common.BottleneckCSP             [64, 64, 1]                   \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  3    161152  models.common.BottleneckCSP             [128, 128, 3]                 \n",
            "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  6                -1  3    641792  models.common.BottleneckCSP             [256, 256, 3]                 \n",
            "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  8                -1  1    656896  models.common.SPP                       [512, 512, [5, 9, 13]]        \n",
            "  9                -1  1   1248768  models.common.BottleneckCSP             [512, 512, 1, False]          \n",
            " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1    378624  models.common.BottleneckCSP             [512, 256, 1, False]          \n",
            " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     95104  models.common.BottleneckCSP             [256, 128, 1, False]          \n",
            " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1    313088  models.common.BottleneckCSP             [256, 256, 1, False]          \n",
            " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1   1248768  models.common.BottleneckCSP             [512, 512, 1, False]          \n",
            " 24      [17, 20, 23]  1     35061  models.yolo.Detect                      [8, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
            "custom_YOLOv5s summary: 283 layers, 7273973 parameters, 7273973 gradients, 17.0 GFLOPs\n",
            "\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "Scaled weight_decay = 0.0005\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD with parameter groups 59 weight (no decay), 70 weight, 62 bias\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(always_apply=False, p=0.01, blur_limit=(3, 7)), MedianBlur(always_apply=False, p=0.01, blur_limit=(3, 7)), ToGray(always_apply=False, p=0.01), CLAHE(always_apply=False, p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '/content/yolov5/CV-lecture-test-2/train/labels' images and labels...456 found, 0 missing, 0 empty, 2 corrupt: 100% 456/456 [00:00<00:00, 977.87it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: /content/yolov5/CV-lecture-test-2/train/images/color1582019439-3968973-0_png.rf.4fdfa14c5c6567ccd3ef4aaa3f6f4207.jpg: ignoring corrupt image/label: only integer scalar arrays can be converted to a scalar index\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: /content/yolov5/CV-lecture-test-2/train/images/color1582019439-3968973-0_png.rf.bb3d6e895973e9b8996c8facdecdd5a5.jpg: ignoring corrupt image/label: only integer scalar arrays can be converted to a scalar index\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/yolov5/CV-lecture-test-2/train/labels.cache\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.2GB ram): 100% 454/454 [00:02<00:00, 186.71it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning '/content/yolov5/CV-lecture-test-2/valid/labels' images and labels...22 found, 0 missing, 0 empty, 1 corrupt: 100% 22/22 [00:00<00:00, 238.08it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING: /content/yolov5/CV-lecture-test-2/valid/images/color1582019423-1230621-0_png.rf.8e34a349a1e56b52641bb3ba3b3b3f0f.jpg: ignoring corrupt image/label: only integer scalar arrays can be converted to a scalar index\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/yolov5/CV-lecture-test-2/valid/labels.cache\n",
            "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.0GB ram): 100% 21/21 [00:00<00:00, 44.65it/s]\n",
            "Plotting labels to runs/train/yolov5s_results/labels.jpg... \n",
            "\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m4.83 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset ✅\n",
            "Image sizes 416 train, 416 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/train/yolov5s_results\u001b[0m\n",
            "Starting training for 300 epochs...\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     0/299      1.8G    0.1013    0.1337   0.05881       113       416: 100% 29/29 [00:15<00:00,  1.88it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:01<00:00,  1.78s/it]\n",
            "                 all         21          0          0          0          0          0\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     1/299     1.99G   0.09369    0.1394   0.05812       168       416: 100% 29/29 [00:11<00:00,  2.46it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:01<00:00,  1.07s/it]\n",
            "                 all         21        316    0.00137     0.0638    0.00128   0.000391\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     2/299     1.99G   0.09056     0.136   0.05705       173       416: 100% 29/29 [00:12<00:00,  2.34it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95:   0% 0/1 [00:00<?, ?it/s]WARNING: NMS time limit 0.930s exceeded\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:01<00:00,  1.36s/it]\n",
            "                 all         21        316     0.0241     0.0588     0.0154    0.00382\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     3/299     1.99G   0.08907    0.1216   0.05547       139       416: 100% 29/29 [00:11<00:00,  2.50it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:01<00:00,  1.01s/it]\n",
            "                 all         21        316     0.0169      0.365     0.0534     0.0136\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     4/299     1.99G   0.08678    0.1237   0.05387       134       416: 100% 29/29 [00:11<00:00,  2.47it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:01<00:00,  1.11s/it]\n",
            "                 all         21        316      0.641     0.0848     0.0264    0.00675\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     5/299     1.99G   0.08375    0.1227   0.05235       198       416: 100% 29/29 [00:11<00:00,  2.52it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  1.02it/s]\n",
            "                 all         21        316     0.0249        0.1       0.03     0.0103\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     6/299     1.99G   0.08153    0.1218   0.05133       167       416: 100% 29/29 [00:11<00:00,  2.55it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  1.58it/s]\n",
            "                 all         21        316      0.439     0.0648     0.0461     0.0135\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     7/299     1.99G   0.07929    0.1271   0.05009       184       416: 100% 29/29 [00:12<00:00,  2.40it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  1.47it/s]\n",
            "                 all         21        316      0.253      0.122     0.0532     0.0136\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     8/299     1.99G   0.07767    0.1301     0.049       130       416: 100% 29/29 [00:11<00:00,  2.45it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95:   0% 0/1 [00:00<?, ?it/s]WARNING: NMS time limit 0.930s exceeded\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:01<00:00,  1.21s/it]\n",
            "                 all         21        316      0.389     0.0934     0.0528     0.0116\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     9/299     1.99G    0.0757    0.1241   0.04812       204       416: 100% 29/29 [00:11<00:00,  2.50it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95:   0% 0/1 [00:00<?, ?it/s]WARNING: NMS time limit 0.930s exceeded\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:01<00:00,  1.37s/it]\n",
            "                 all         21        316      0.385      0.134      0.109     0.0331\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "    10/299     1.99G   0.07567    0.1206   0.04695       162       416: 100% 29/29 [00:11<00:00,  2.48it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  1.02it/s]\n",
            "                 all         21        316      0.427      0.195      0.134     0.0374\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "    11/299     1.99G   0.07242    0.1197   0.04573       149       416: 100% 29/29 [00:11<00:00,  2.46it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  1.12it/s]\n",
            "                 all         21        316      0.387      0.207      0.139     0.0456\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "    12/299     1.99G   0.07168    0.1203    0.0447       157       416: 100% 29/29 [00:11<00:00,  2.50it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  1.36it/s]\n",
            "                 all         21        316      0.402      0.218      0.224     0.0899\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "    13/299     1.99G   0.07095    0.1193   0.04322       152       416: 100% 29/29 [00:11<00:00,  2.50it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  1.53it/s]\n",
            "                 all         21        316      0.631      0.224      0.228      0.081\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "    14/299     1.99G   0.06843    0.1196   0.04218       159       416: 100% 29/29 [00:11<00:00,  2.48it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:01<00:00,  1.09s/it]\n",
            "                 all         21        316      0.625      0.269      0.275     0.0872\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "    15/299     1.99G   0.06872    0.1159   0.04155       218       416: 100% 29/29 [00:11<00:00,  2.47it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  1.47it/s]\n",
            "                 all         21        316      0.698       0.29      0.307      0.136\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "    16/299     1.99G   0.06849    0.1145   0.04065       110       416: 100% 29/29 [00:11<00:00,  2.48it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:01<00:00,  1.17s/it]\n",
            "                 all         21        316      0.652      0.288      0.255     0.0915\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "    17/299     1.99G     0.066    0.1164   0.03948       140       416: 100% 29/29 [00:11<00:00,  2.48it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  1.73it/s]\n",
            "                 all         21        316      0.698       0.31      0.327      0.121\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "    18/299     1.99G   0.06384    0.1097   0.03837       140       416: 100% 29/29 [00:11<00:00,  2.51it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  1.54it/s]\n",
            "                 all         21        316      0.652      0.298      0.296      0.137\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "    19/299     1.99G   0.06488    0.1141   0.03784       176       416: 100% 29/29 [00:11<00:00,  2.43it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  1.79it/s]\n",
            "                 all         21        316      0.765      0.356      0.395      0.189\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "    20/299     1.99G   0.06313    0.1094   0.03684       110       416: 100% 29/29 [00:13<00:00,  2.21it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  2.10it/s]\n",
            "                 all         21        316      0.733      0.378      0.376      0.166\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "    21/299     1.99G   0.06395    0.1076   0.03617       102       416: 100% 29/29 [00:11<00:00,  2.42it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  1.28it/s]\n",
            "                 all         21        316       0.72       0.31      0.322      0.142\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "    22/299     1.99G   0.06283    0.1137   0.03578       240       416: 100% 29/29 [00:11<00:00,  2.47it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  1.39it/s]\n",
            "                 all         21        316       0.77      0.356      0.458      0.196\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "    23/299     1.99G   0.06135    0.1099   0.03545       208       416: 100% 29/29 [00:11<00:00,  2.47it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  2.18it/s]\n",
            "                 all         21        316      0.742      0.373      0.525      0.211\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "    24/299     1.99G   0.06038    0.1077    0.0341       216       416: 100% 29/29 [00:11<00:00,  2.47it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  1.75it/s]\n",
            "                 all         21        316       0.53      0.546      0.615      0.254\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "    25/299     1.99G   0.05901     0.107   0.03382       142       416: 100% 29/29 [00:11<00:00,  2.52it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  1.86it/s]\n",
            "                 all         21        316      0.772      0.374      0.472      0.207\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "    26/299     1.99G   0.06099    0.1089   0.03368       184       416: 100% 29/29 [00:12<00:00,  2.42it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  1.98it/s]\n",
            "                 all         21        316      0.504      0.617      0.627      0.289\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "    27/299     1.99G   0.05868    0.1043   0.03333       204       416: 100% 29/29 [00:11<00:00,  2.51it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  1.97it/s]\n",
            "                 all         21        316      0.767      0.396      0.539      0.225\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "    28/299     1.99G   0.05724    0.1041   0.03275       152       416: 100% 29/29 [00:11<00:00,  2.53it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  1.70it/s]\n",
            "                 all         21        316      0.767      0.397      0.522      0.241\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "    29/299     1.99G   0.05599    0.1012   0.03255       391       416:  69% 20/29 [00:07<00:03,  2.31it/s]"
          ]
        }
      ],
      "source": [
        "# train yolov5s on custom data for 300 epochs (takes ~1hour, reduce to 100 epochs for ~20mins)\n",
        "# time its performance\n",
        "# Try changing\n",
        "%%time\n",
        "%cd /content/yolov5/\n",
        "!python train.py --img 416 --batch 16 --epochs 300 --data {dataset.location}/data.yaml --cfg ./models/custom_yolov5s.yaml --weights '' --name yolov5s_results  --cache"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJVs_4zEeVbF"
      },
      "source": [
        "# Evaluate Custom YOLOv5 Detector Performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KN5ghjE6ZWh"
      },
      "source": [
        "Training losses and performance metrics are saved to Tensorboard and also to a logfile defined above with the **--name** flag when we train. In our case, we named this `yolov5s_results`. (If given no name, it defaults to `results.txt`.) The results file is plotted as a png after training completes.\n",
        "\n",
        "Note from Glenn: Partially completed `results.txt` files can be plotted with `from utils.utils import plot_results; plot_results()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bOy5KI2ncnWd",
        "outputId": "be6d760d-c917-498a-e96c-18a7f713955b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Launching TensorBoard..."
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Start tensorboard\n",
        "# Launch after you have started training\n",
        "# logs save in the folder \"runs\"\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir runs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "C60XAsyv6OPe",
        "outputId": "c4c92a8c-18fd-4982-d199-b8fc12c0ce82"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-d7ea088d00dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# we can also output some older school graphs if the tensor board isn't working for whatever reason...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplots\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot_results\u001b[0m  \u001b[0;31m# plot results.txt as results.png\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/content/yolov5/runs/train/yolov5s_results/results.png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# view results.png\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# we can also output some older school graphs if the tensor board isn't working for whatever reason... \n",
        "from utils.plots import plot_results  # plot results.txt as results.png\n",
        "Image(filename='/content/yolov5/runs/train/yolov5s_results/results.png', width=1000)  # view results.png"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLI1JmHU7B0l"
      },
      "source": [
        "### Curious? Visualize Our Training Data with Labels\n",
        "\n",
        "After training starts, view `train*.jpg` images to see training images, labels and augmentation effects.\n",
        "\n",
        "Note a mosaic dataloader is used for training (shown below), a new dataloading concept developed by Glenn Jocher and first featured in [YOLOv4](https://arxiv.org/abs/2004.10934)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "PF9MLHDb7tB6",
        "outputId": "9c522821-e503-47e1-ffcd-fb24a6f43220"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GROUND TRUTH TRAINING DATA:\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-38e740409edb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# first, display our ground truth data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GROUND TRUTH TRAINING DATA:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/content/yolov5/CV-lecture-test-2/train/images/color1582019439-3968973-0_png.rf.4fdfa14c5c6567ccd3ef4aaa3f6f4207.jpg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m900\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, url, filename, format, embed, width, height, retina, unconfined, metadata)\u001b[0m\n\u001b[1;32m   1201\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretina\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretina\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munconfined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munconfined\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1203\u001b[0;31m         super(Image, self).__init__(data=data, url=url, filename=filename, \n\u001b[0m\u001b[1;32m   1204\u001b[0m                 metadata=metadata)\n\u001b[1;32m   1205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, url, filename, metadata)\u001b[0m\n\u001b[1;32m    625\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mreload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1233\u001b[0m         \u001b[0;34m\"\"\"Reload the raw data from file or URL.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretina\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retina_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mreload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    650\u001b[0m         \u001b[0;34m\"\"\"Reload the raw data from file or URL.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 652\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_flags\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    653\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/yolov5/CV-lecture-test-2/train/images/color1582019439-3968973-0_png.rf.4fdfa14c5c6567ccd3ef4aaa3f6f4207.jpg'"
          ]
        }
      ],
      "source": [
        "# first, display our ground truth data\n",
        "print(\"GROUND TRUTH TRAINING DATA:\")\n",
        "Image(filename='/content/yolov5/CV-lecture-test-2/train/images/color1582019439-3968973-0_png.rf.4fdfa14c5c6567ccd3ef4aaa3f6f4207.jpg', width=900)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "W40tI99_7BcH",
        "outputId": "6d475769-898a-4150-a78d-784a52329753"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GROUND TRUTH AUGMENTED TRAINING DATA:\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-f6c08eb94f11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# print out an augmented training example\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GROUND TRUTH AUGMENTED TRAINING DATA:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/content/yolov5/CV-lecture-test-2/train/images/color1582019439-3968973-0_png.rf.4fdfa14c5c6567ccd3ef4aaa3f6f4207.jpg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m900\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, url, filename, format, embed, width, height, retina, unconfined, metadata)\u001b[0m\n\u001b[1;32m   1201\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretina\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretina\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munconfined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munconfined\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1203\u001b[0;31m         super(Image, self).__init__(data=data, url=url, filename=filename, \n\u001b[0m\u001b[1;32m   1204\u001b[0m                 metadata=metadata)\n\u001b[1;32m   1205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, url, filename, metadata)\u001b[0m\n\u001b[1;32m    625\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mreload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1233\u001b[0m         \u001b[0;34m\"\"\"Reload the raw data from file or URL.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretina\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retina_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mreload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    650\u001b[0m         \u001b[0;34m\"\"\"Reload the raw data from file or URL.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 652\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_flags\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    653\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/yolov5/CV-lecture-test-2/train/images/color1582019439-3968973-0_png.rf.4fdfa14c5c6567ccd3ef4aaa3f6f4207.jpg'"
          ]
        }
      ],
      "source": [
        "# print out an augmented training example\n",
        "print(\"GROUND TRUTH AUGMENTED TRAINING DATA:\")\n",
        "Image(filename='/content/yolov5/CV-lecture-test-2/train/images/color1582019439-3968973-0_png.rf.4fdfa14c5c6567ccd3ef4aaa3f6f4207.jpg', width=900)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3qM6T0W53gh"
      },
      "source": [
        "#Run Inference  With Trained Weights\n",
        "Run inference with a pretrained checkpoint on contents of `test/images` folder downloaded from Roboflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yIEwt5YLeQ7P",
        "outputId": "4f27afc6-386f-41d6-a8be-fff68ca591b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ls: cannot access 'runs/': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "# trained weights are saved by default in our weights folder\n",
        "%ls runs/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4SyOWS80qR32",
        "outputId": "f63d20b6-7e22-40b0-87d7-e056d758c2e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ls: cannot access 'runs/train/yolov5s_results/weights': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "%ls runs/train/yolov5s_results/weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9nmZZnWOgJ2S",
        "outputId": "38b19e72-091b-4b0f-de6c-a89db5ab1c9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: '/content/yolov5/'\n",
            "/content\n",
            "python3: can't open file 'detect.py': [Errno 2] No such file or directory\n"
          ]
        }
      ],
      "source": [
        "# when we ran this, we saw .007 second inference time. That is 140 FPS on a TESLA P100!\n",
        "# use the best weights!\n",
        "%cd /content/yolov5/\n",
        "!python detect.py --weights runs/train/yolov5s_results/weights/best.pt --img 416 --conf 0.6 --source /content/yolov5/CV-lecture-test-2/test/images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "odKEqYtTgbRc"
      },
      "outputs": [],
      "source": [
        "#display inference on ALL test images\n",
        "#this looks much better with longer training above\n",
        "\n",
        "import glob\n",
        "from IPython.display import Image, display\n",
        "\n",
        "for imageName in glob.glob('/content/yolov5/runs/detect/exp/*.jpg'): #assuming JPG\n",
        "    display(Image(filename=imageName))\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uPq9mVgiBql"
      },
      "source": [
        "# Export Trained Weights for Future Inference\n",
        "\n",
        "Now that you have trained your custom detector, you can export the trained weights you have made here for inference on your device elsewhere"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YH4CTzDRh00g",
        "outputId": "2e391b5f-482c-4ee2-83a0-23d8d6ebbe83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1x_wg3VeiXMW",
        "outputId": "b1266882-f6de-48e1-ccfe-3c8b8055c795"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cp: cannot stat '/content/yolov5/runs/train/yolov5s_results/weights/best.pt': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "%cp /content/yolov5/runs/train/yolov5s_results/weights/best.pt /content/gdrive/My\\ Drive\n",
        "#%cp -r /content/yolov5/ /content/gdrive/My\\ Drive/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVpCFeU-K4gb"
      },
      "source": [
        "## That's all folks! \n",
        "\n",
        "Try with you own datasets and different settings!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}